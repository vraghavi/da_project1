{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Decision Trees and k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Decision Tree \n",
    "We begin our overview of classification methods with discussing decision trees. To build a decision tree, we can use any choice of impurity measure (e.g., Entropy, Gini, or misclassification error) and select an attribute for splitting at an internal node that provides the maximum gain in the impurity measure at the collection of children nodes obtained after splitting. Here is a plot of how different impurity measures vary with the probability of the positive class (p+) at any node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't like warnings\n",
    "# you can comment the following 2 lines if you'd like to\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "from matplotlib import pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "xx = np.linspace(0,1,50)\n",
    "plt.plot(xx, [2 * x * (1-x) for x in xx], label='gini')\n",
    "plt.plot(xx, [-x * np.log2(x) - (1-x) * np.log2(1 - x)  for x in xx], label='entropy')\n",
    "plt.plot(xx, [1 - max(x, 1-x) for x in xx], label='missclass')\n",
    "plt.xlabel('p+')\n",
    "plt.ylabel('criterion')\n",
    "plt.title('Criteria of quality as a function of p+ (binary classification)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree using Scikit-learn\n",
    "Let's consider fitting a decision tree to some synthetic data. We will generate samples from two classes, both normal distributions but with different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first class\n",
    "np.random.seed(17)\n",
    "train_data = np.random.normal(size=(100, 2))\n",
    "train_labels = np.zeros(100)\n",
    "\n",
    "# adding second class\n",
    "train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)]\n",
    "train_labels = np.r_[train_labels, np.ones(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(train_data[:, 0], \n",
    "            train_data[:, 1], c=train_labels, s=100, \n",
    "cmap='autumn', edgecolors='black', linewidth=1.5);\n",
    "plt.plot(range(-2,5), range(4,-3,-1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, the goal of classification here is to build a \"good\" decision boundary separating the two classes (the red dots from the yellow). A straight line will be too simple while some complex curve snaking by each red dot will be too complex and will lead us to making mistakes on new \"unseen\" samples. Intuitively, some smooth boundary would work well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class DecisionTreeClassifier in Scikit-learn\n",
    "The main hyper-parameters of the [`sklearn.tree.DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) class are:\n",
    "\n",
    "- `max_depth` – the maximum depth of the tree;\n",
    "- `criterion` - {“gini”, “entropy”}, default=”gini”\n",
    "- `max_features` - the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be \"expensive\" to search for partitions for *all* features);\n",
    "- `min_samples_leaf` – the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few training instances.\n",
    "\n",
    "The parameters of the tree need to be set depending on input data, and it is usually done by means of *cross-validation*, more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to separate the two classes of the synthetic dataset by training an `Sklearn` decision tree. We will use `max_depth` parameter that limits the depth of the tree. Let's visualize the resulting separating boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pydotplus #To install this package, run: conda install -c anaconda pydotplus\n",
    "from sklearn.tree import export_graphviz #To install this package, run: conda install -c anaconda graphviz\n",
    "\n",
    "# Let’s write an auxiliary function that will return grid for further visualization.\n",
    "\n",
    "def tree_graph_to_png(tree, feature_names, png_file_to_save):\n",
    "    tree_str = export_graphviz(tree, feature_names=feature_names, \n",
    "                                     filled=True, out_file=None)\n",
    "    graph = pydotplus.graph_from_dot_data(tree_str)  \n",
    "    graph.write_png(png_file_to_save)\n",
    "    \n",
    "    \n",
    "def get_grid(data):\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "\n",
    "#For the assignment, you have to change max_depth parameter and re-run this cell \n",
    "\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy',max_depth= 3, random_state=17)\n",
    "\n",
    "# training the tree\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "\n",
    "# some code to depict separating surface\n",
    "xx, yy = get_grid(train_data)\n",
    "predicted = clf_tree.predict(np.c_[xx.ravel(), \n",
    "                                   yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='autumn')\n",
    "plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, \n",
    "            cmap='autumn', edgecolors='black', linewidth=1.5);\n",
    "\n",
    "\n",
    "tree_graph_to_png(tree=clf_tree, feature_names=['x1', 'x2'], png_file_to_save='topic3_decision_tree1.png')\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"topic3_decision_tree1.png\"\n",
    "Image(filename = PATH , width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how does the tree itself look? We see that the tree \"cuts\" the space into 8 rectangles for max depth = 3, i.e. the tree has 8 leaves. Within each rectangle, the tree will make the prediction according to the majority label of the objects inside it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we \"read\" such a tree?\n",
    "**Explanation for tree with max_depth = 3**\n",
    "In the beginning, there were 200 samples (instances), 100 of each class. The entropy at this stage was maximal, $S=1$. Then, the first partition of the samples into 2 groups was made by comparing the value of $x_2$ with $1.211$ (find this part of the border in the picture above). With that, the entropy of both left and right groups decreased. The process continues up to depth 3. In this visualization, the more samples of the first class, the darker the orange color of the vertex; the more samples of the second class, the darker the blue. At the beginning, the number of samples from two classes is equal, so the root node of the tree is white."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a Decision Tree Works with Numerical Features  [Bank Dataset]\n",
    "\n",
    "Suppose we have a numeric feature \"Age\" that has a lot of unique values. A decision tree will look for the best split by checking binary attributes such as \"Age <17\", \"Age < 22.87\", and so on. But what if the age range is large? Or what if another quantitative variable, \"salary\", can also be \"cut\" in many ways? There will be too many binary attributes to select from at each step during tree construction. To resolve this problem, heuristics are usually used to limit the number of thresholds to which we compare the quantitative variable.\n",
    " \n",
    "Let's consider an example. Suppose we have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Age': [17,64,18,20,38,49,55,25,29,31,33],\n",
    "                     'Loan Default': [1,0,1,0,1,0,0,1,1,0,1]})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort it by age in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tree = DecisionTreeClassifier(random_state=17)\n",
    "age_tree.fit(data['Age'].values.reshape(-1, 1), data['Loan Default'].values)\n",
    "\n",
    "tree_graph_to_png(age_tree, feature_names=['Age'], \n",
    "                 png_file_to_save='topic3_decision_tree2.png')\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"topic3_decision_tree2.png\"\n",
    "Image(filename = PATH , width=500, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the tree used the following 5 values to evaluate by age: 43.5, 19, 22.5, 30, and 32 years. If you look closely, these are exactly the mean values between the ages at which the target class \"switches\" from 1 to 0 or 0 to 1. To illustrate further, 43.5 is the average of 38 and 49 years; a 38-year-old customer failed to return the loan whereas the 49-year-old did. The tree looks for the values at which the target class switches its value as a threshold for \"cutting\" a quantitative variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest Neighbors Method\n",
    "\n",
    "*The nearest neighbors method* (k-Nearest Neighbors, or k-NN) is another very popular classification method. The quality of classification with k-NN depends on several parameters:\n",
    "\n",
    "- The number of neighbors $k$.\n",
    "- The distance measure between samples (common ones include Hamming, Euclidean, cosine, and Minkowski distances). Note that most of these metrics require data to be scaled. Simply speaking, we do not want the \"salary\" feature, which is on the order of thousands, to affect the distance more than \"age\", which is generally less than 100. \n",
    "- Weights of neighbors (each neighbor may contribute different weights; for example, the further the sample, the lower the weight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class `KNeighborsClassifier` in Scikit-learn\n",
    "The main hyper-parameters of the class `sklearn.neighbors.KNeighborsClassifier` are:\n",
    "- weights: `uniform` (all weights are equal), `distance` (the weight is inversely proportional to the distance from the test sample), or any other user-defined function;\n",
    "- algorithm (optional): `brute`, `ball_tree `, `KD_tree`, or `auto`. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to `auto`, the right way to find the neighbors will be automatically chosen based on the training set.\n",
    "- leaf_size (optional): threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;\n",
    "- metric: `minkowski`, `manhattan `, `euclidean`, `chebyshev`, or other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing Model Parameters and Cross-Validation \n",
    "\n",
    "The main task of learning algorithms is to be able to *generalize* to unseen data. Since we cannot immediately check the model performance on new, incoming data (because we do not know the true values of the target variable yet), it is necessary to sacrifice a small portion of the data to check the quality of the model on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is often done in one of two ways:\n",
    "- setting aside a part of the dataset (*held-out/hold-out set*). Thus we reserve a fraction of the training set (typically from 20% to 40%), train the model on the remaining data (60-80% of the original set), and compute performance metrics for the model (e.g accuracy) on the hold-out set.\n",
    "- *cross-validation*. The most frequent case here is *k-fold cross-validation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/webt/80/nx/1p/80nx1pa4iet33x9pw-bj02khyhs.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In k-fold cross-validation, the model is trained $K$ times on different ($K-1$) subsets of the original dataset (in white) and checked on the remaining subset (each time a different one, shown above in orange).\n",
    "We obtain $K$ model quality assessments that are usually averaged to give an overall average quality of classification.\n",
    "\n",
    "Cross-validation provides a better assessment of the model quality on new data compared to the hold-out set approach. However, cross-validation is computationally expensive when you have a lot of data.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application Examples\n",
    "\n",
    "### Decision trees and nearest neighbors method in a customer churn prediction task \n",
    "\n",
    "Let's read data into a `DataFrame` and preprocess it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('telecom_churn.csv')\n",
    "\n",
    "df['international plan'] = pd.factorize(df['international plan'])[0]\n",
    "df['voice mail plan'] = pd.factorize(df['voice mail plan'])[0]\n",
    "df['churn'] = df['churn'].astype('int')\n",
    "states = df['state']\n",
    "y = df['churn']\n",
    "df.drop(['state', 'churn','phone number'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's allocate 70% of the set for training (`X_train`, `y_train`) and 30% for the hold-out set (`X_holdout`, `y_holdout`). The hold-out set will not be involved in tuning the hyper-parameters of the models. We'll use it at the end, after tuning, to assess the quality of the resulting model. Let's train 2 models: decision tree and k-NN. We do not know what hyper-parameters are good, so we will assume some random ones: a tree depth of 5 and the number of nearest neighbors equal 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(df.values, y, test_size=0.3,\n",
    "random_state=17)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=17)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# for kNN, we need to scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "knn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assess prediction quality on our hold-out set with a simple metric, the proportion of correct answers (accuracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree_pred = tree.predict(X_holdout)\n",
    "print (f\" Accuracy for decision tree is {accuracy_score(y_holdout, tree_pred)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d  = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "d_scaled = scaler.fit_transform(d)\n",
    "d_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn.predict(X_holdout_scaled)\n",
    "print (f\" Accuracy for k-nn is {accuracy_score(y_holdout, knn_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree did better: the percentage of correct answers is about 91% (decision tree) versus 88% (k-NN). Note that this performance is achieved by using randomly selected hyper-parameter values.\n",
    "\n",
    "Now, let's identify the hyper-parameters for the tree using cross-validation. We'll tune the maximum depth and the maximum number of features used at each split using `GridSearchCV`. Here is how it works: for each unique pair of values of `max_depth` and `max_features`, it compute model performance with 5-fold cross-validation, and then select the best combination of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "tree_params = {'max_depth': range(1,11),\n",
    "               'max_features': range(4,19)}\n",
    "\n",
    "tree_grid = GridSearchCV(tree, tree_params, cv=5, n_jobs=-1, verbose=True)\n",
    "\n",
    "tree_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list the best parameters and the corresponding mean accuracy from cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Best parameters are {tree_grid.best_params_}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_holdout, tree_grid.predict(X_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the resulting tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph_to_png(tree=tree_grid.best_estimator_, feature_names=df.columns, png_file_to_save='topic3_decision_tree4.png')\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"topic3_decision_tree4.png\"\n",
    "Image(filename = PATH , width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that it is not entirely a toy example (its maximum depth is 6), the picture is not that small, but you can zoom in the picture. It is saved in the directory with the name 'topic3_decision_tree4.png', from where you are running this notebook. \n",
    "\n",
    "Now, let's tune the number of neighbors $k$ for k-NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n",
    "\n",
    "knn_params = {'knn__n_neighbors': range(1, 10)}\n",
    "\n",
    "knn_grid = GridSearchCV(knn_pipe, knn_params,\n",
    "                        cv=5, n_jobs=-1, verbose=True)\n",
    "\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "knn_grid.best_params_, knn_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_holdout, knn_grid.predict(X_holdout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the tree proved to be better than the nearest neighbors algorithm: 94.2%/93.6% accuracy for cross-validation and hold-out respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Case for Decision Trees\n",
    "\n",
    "To continue the discussion of the pros and cons of the two methods in question, let's consider a simple classification task, where a tree would perform well but does it in an \"overly complicated\" manner. Let's create a set of points on a plane (2 features), each point will be one of two classes (+1 for red, or -1 for yellow). If you look at it as a classification problem, it seems very simple: the classes are separated by a line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_linearly_separable_data(n=500, x1_min=0, x1_max=30, \n",
    "                                 x2_min=0, x2_max=30):\n",
    "    data, target = [], []\n",
    "    for i in range(n):\n",
    "        x1 = np.random.randint(x1_min, x1_max)\n",
    "        x2 = np.random.randint(x2_min, x2_max)\n",
    "        if np.abs(x1 - x2) > 0.5:\n",
    "            data.append([x1, x2])\n",
    "            target.append(np.sign(x1 - x2))\n",
    "    return np.array(data), np.array(target)\n",
    "\n",
    "X, y = form_linearly_separable_data()\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='autumn', edgecolors='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the border that the decision tree builds is too complicated; plus the tree itself is very deep. Also, imagine how badly the tree will generalize to the space beyond the $30 \\times 30$ squares that frame the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=17).fit(X, y)\n",
    "\n",
    "xx, yy = get_grid(X)\n",
    "predicted = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='autumn')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, \n",
    "            cmap='autumn', edgecolors='black', linewidth=1.5)\n",
    "plt.title('Easy task. Decision tree compexifies everything');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got this overly complex construction, although the solution is just a straight line $x_1 = x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph_to_png(tree=tree, feature_names=['x1', 'x2'], png_file_to_save='topic3_decision_tree5.png')\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "PATH = \"topic3_decision_tree5.png\"\n",
    "Image(filename = PATH , width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of one nearest neighbor does better than the tree. The following cell will take approx 4-5 minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1,n_jobs = -1).fit(X, y)\n",
    "\n",
    "xx, yy = get_grid(X)\n",
    "predicted = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, predicted, cmap='autumn')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, \n",
    "            cmap='autumn', edgecolors='black', linewidth=1.5);\n",
    "plt.title('Easy task, kNN. Not bad');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Decision Trees and k-NN in a Task of MNIST Handwritten Digits Recognition\n",
    "\n",
    "Now let's have a look at how these 2 algorithms perform on a real-world task. We will use the `sklearn` built-in dataset on handwritten digits. This task is an example where k-NN works surprisingly well.\n",
    " \n",
    "Pictures here are 8x8 matrices (intensity of white color for each pixel). Then each such matrix is \"unfolded\" into a vector of length 64, and we obtain a feature description of an object.\n",
    " \n",
    "Let's draw some handwritten digits. We see that they are distinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X[0,:].reshape([8,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 4, sharey=True, figsize=(16,6))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(X[i,:].reshape([8,8]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's do the same experiment as in the previous task, but, this time, let's change the ranges for tunable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select 70% of the dataset for training (`X_train`, `y_train`) and 30% for holdout (`X_holdout`, `y_holdout`). The holdout set will not participate in hyper-parameters tuning; we will use it at the end to check the quality of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3,random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train a decision tree and k-NN with our random hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, random_state=17)\n",
    "knn_pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('knn', KNeighborsClassifier(n_neighbors=10))])\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "knn_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s make predictions on our holdout set. We can see that k-NN did much better, but note that this is with random hyper-parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pred = tree.predict(X_holdout)\n",
    "knn_pred = knn_pipe.predict(X_holdout)\n",
    "print (f\"K-Nearest neighbor accuracy is {accuracy_score(y_holdout, knn_pred)}\")\n",
    "print (f\"Decision tree accuracy is {accuracy_score(y_holdout, tree_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s tune our hyper-parameters using cross-validation as before, but now we’ll take into account that we have more features than in the previous task: 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {'max_depth': [1, 2, 3, 5, 10, 20, 25, 30, 40, 50, 64],\n",
    "               'max_features': [1, 2, 3, 5, 10, 20 ,30, 50, 64]}\n",
    "\n",
    "tree_grid = GridSearchCV(tree, tree_params, \n",
    "                         cv=5, n_jobs=-1, verbose=True)\n",
    "\n",
    "tree_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's see the best parameters combination and the corresponding accuracy from cross-validation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_grid.best_params_, tree_grid.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"Decision tree accuracy is {accuracy_score(y_holdout,  tree_grid.predict(X_holdout))}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has already passed 66% but not quite 97%. kNN works better on this dataset. In the case of one nearest neighbor, we were able to reach 99% guesses on cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=1), X_train, y_train, cv=5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results\n",
    "*(Legend: CV and Holdout are average shares of the correct answers on cross-model validation and hold-out sample. DT stands for a decision tree, k-NN stands for k-nearest neighbors.\n",
    "\n",
    "|   algo\\eval      |   CV  | Holdout |  \n",
    "|---------|-------|---------|\n",
    "| **DT**  | 0.844 |  0.838  |  \n",
    "| **kNN** | 0.987 |  0.983  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **conclusion** of this experiment (and general advice): first check simple models on your data: decision tree and nearest neighbors. It might be the case that these methods already work well enough.\n",
    "\n",
    "\n",
    "This notebook is adaption from the following kaggle notebook https://www.kaggle.com/kashnitsky/topic-3-decision-trees-and-knn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
